<?xml version="1.0" encoding="UTF-8" ?>
<subject>
<name>Aude Billard</name>
<interview>
<interviewer>If you could just start by telling us where you were born and where you grew up and went to school.</interviewer>
<interviewee>So I was born here, in fact, in Lausanne, and I grew up here, and I went to school, all of my high school here in Lausanne.  And then when I started university, I started university here also at EPFL, and then I moved to Zurich for an exchange, at the ETH Zurich.  So I was studying physics there.  And after graduating from the bachelor/master in physics, then I went to the University of Edinburgh, where I did my PhD.</interviewee>
<interviewer>Who did you work with at Edinburgh?</interviewer>
<interviewee>So, at Edinburgh I worked mostly with Gillian Hayes and John Hallam.  </interviewee>
<interviewer>What kind of projects were you working on?</interviewer>
<interviewee>That was robotics.  It was the early days of  apply to robotics.  So that was my thesis, basically.  Neural networks.  I also worked partly with David Willshaw, who was more on the connectionist side.  And I also had a close interaction with Alan Bundy when I was doing my master, was more in automated reasoning, a field that I was interested in, but finally I went to robotics.  I mean, automated reasoning is more the theoretical part of machine learning, and so finally I opted for the more practical application of robotics. </interviewee>
<interviewer>What year did you get to Edinburgh?</interviewer>
<interviewee>How did I get there?  Well, , I was looking for a way to practice my English, go away from Switzerland, but I was not eager to go to the U.S.    So Edinburgh seemed like a good compromise.  It was a great school, highly ranked.  It was-- I mean, the way they advertised for their program in artificial intelligence was to say that they were the sole department with the MIT that was offering that throughout the world.  I actually had a scholarship to go to MIT, but I turned that down-- -- to go to Edinburgh.  Shall I say that, by pure prejudices against the U.S., as an European?  And yeah.  I mean, the University of Edinburgh is a great place to be.  So I have no regret over that.  It was a great place to be, great people, and yeah, and it's  beautiful country.  </interviewee>
<interviewer>Who were some of the other graduate students who were in the program when you were there?</interviewer>
<interviewee>Yeah, that's-- so, Yiannis Demiris is one, and he's currently senior lecturer at Imperial College, London.  And so he was one year senior from me, and he actually was the one who suggested .  So that's the closest person I worked with.  There were a few others, but I don't know their name.  I mean, I cannot recall their right now.</interviewee>
<interviewer>And what was the state of robotics research at Edinburgh when you arrived there?  Were there a lot of people working in that area?</interviewer>
<interviewee>Yeah, I think it was really one of these departments that were a bit unique in this respect because they were putting a lot of money-- and especially for the U.K.-- in terms of just hardware.  So they had a workshop with about ten people that were working there to build robots, to help us build robots.  And there are a few institutions that actually afford that, so it was-- I mean, they really wanted to have robots.  They were also trying to purchase robots out of the shelf, but of course it was difficult because at the time-- we had the first robots called Gillespie, which is this robot wheel-based, but it was before the pioneer robots came out.  Myself, I had to entirely build robots because we couldn't-- I mean, I wanted to have a humanoid robots, and that didn't exist except for Cog, so I built a Lego type of robot.  I used Lego to build a small humanoid robot, to just have a platform that will fit my needs.  The state of robotics at the time was all about wheel-based robots, sort of car-like robots.  And maybe because I'm a girl, I was not too much into car-like robots.  I thought it was a bit-- especially I was interested in artificial intelligence in more cognitive processes, so with a car, it's really difficult to try to have some complexity.  I wanted to teach language to robots, and two cars that talked to each other is-- -- is a bit odd.  Yeah.  </interviewee>
<interviewer>Were many people trying to apply neural networks to the machine learning techniques?</interviewer>
<interviewee>Say that again, sorry?</interviewee>
<interviewer>Were other people trying to use neural networks research with robotics?</interviewer>
<interviewee>That's correct.  So neural networks were a very hot topic, bio-inspired type of robotics.  At the time, at least in this department, was really sort of seen as the way to go.  So there were different ways of being bio-inspired.  Some people did a lot of reinforcement learning, which was already seen as bio-inspired, just from the idea of real-world [ph?] signal.  But there was this group, this large group, with David Willshaw and a lot of people that were trying to model the brain from a connectionist viewpoint.  And since they were already next door, there were a lot of interaction between the AI people and these people.  We were very inspired also by concepts from psychology.  Hot topics were modeling language of bees, insect type of behavior, collective robotics.  So yes, connectionist models in general were viewed as one approach that one should pursue.  So some of the graduate students were looking at genetic algorithms as one way to evolve the neural network.  I personally took more of a theoretical road.  I was trained in physics, so I was more trying to develop the math behind the network itself, how we could exploit temporal and spatial information at best, and changing classical Hebbian to increase the capacity of the network.  So I was taking a more systematic approach to this, which of course scaled down the scope of the application of the neural network that I could do compared to people who were using genetic algorithm and had a larger variety of application and complexity in the type of application.</interviewee>
<interviewer>And what was the application?  What were you trying to get the robot to learn?</interviewer>
<interviewee>So, the application that I did was to have a robot learn from human guidance a simple vocabulary.  And so you would have either a leader robot that knew how to label all sort of things or places or direction or commands-- motor commands, going left and right.  You had a leader robot and then you had a follower robot, so you had two robots that will follow each other, and the follower, which is a little bit like a baby following a mother, will hear the mother talk and try to associate its perception of the environment with the speech of the mother.  So it was the early days of what is called a simple grounding problem-- how could you ground words into various perception from the viewpoint of a robot?  And this perception could entail sensory perception as well as motor perception.  And there were basic problems, such as simply the mother being a bit ahead-- I mean, seeing things before the follower or the child will see them; will speak before the child will actually see or perceive the same thing.  And this is what you find in nature too.  So I thought it was very interesting to try to embed in a connectionist how does the brain, in effect, use this temporal delay between-- we talk about drawing attention.  We say that we both look at the same thing at the same time.  It's not true; there is always some type of delay.  How do we encapsulate that in a relatively generic manner?  So the application was language.  We also worked, together with Kerstin Dautenhahn, who became then my second official advisor, even though she wasn't in Edinburgh at the time.  We looked at how we could apply the same type of algorithm to teach a humanoid robot-- so a little doll robot-- a language.  And so basically the application was grounding, simple grounding, or teaching a simple vocabulary.  It's not really language; it's a proto-language.</interviewee>
<interviewer>But there weren't a lot of people working in language learning within robotic systems at that time.</interviewer>
<interviewee>No.  The only person who had done something at the time was Holly Yanco, during her master's thesis at the MIT.  But there were a lot of people talking about language and trying to develop model, but it was mostly linguistic people.  So you had Steven Pinker, had written a very important book.  You had Bickerton.  You had a number of people.  And these people also came to U.K., came to Edinburgh, so if you want to-- the spirit was there, but in robotics, no, very few people were looking at that.</interviewee>
<interviewer>And what year did you finish your PhD?</interviewer>
<interviewee>I finished my PhD very rapidly.    In fact, I did my PhD in really exactly the amount of time that was obligatory-- so three years, but that includes also the master's, so it was really-- I finished in '98.  In September '98.</interviewee>
<interviewer>And where did you go from there?</interviewer>
<interviewee>I was actually contacted by Maja Mataric for doing a postdoc with her, but then for six months I first came back to Switzerland, in between-- we were formalizing the contract to go to USC.  So I worked here in Switzerland at the EPFL with Professor Nicoud.  We actually  to create a startup that is now studying the robot doll that I created during my thesis.  And then I went for a postdoc with my Maja Mataric at the University of Southern California for a year.  And then I was promoted as an assistant professor there, and I worked together with Stefan Schaal and Michael Arbib.</interviewee>
<interviewer>So the doll that you worked on here with Nicoud, that was an experimental doll or a custom doll, or--?</interviewer>
<interviewee>No, so-- yeah, we-- so I had built a doll that was made of Lego when I was in , and then when Jean Nicoud saw that, he said, "Oh, we need to redesign that.  We should be able to miniaturize that.  We have everything at EPFL.  We have workshop for this."  And so I wanted to really keep the aesthetic aspect of the doll, and so I simply went to shops and purchased a classical doll that you can find, and took everything out-- -- and then replaced that with-- well, we kept some of the original motors.  We changed some of the motors, we changed the gearing, and we changed the electronics to enable full control while keeping the complete aesthetic of the doll that is created by classical manufacturer.  So.</interviewee>
<interviewer>Did you try to encode facial expression?</interviewer>
<interviewee>That would have meant to redesign entirely the doll, because it's very difficult.  You need to have skin.  So I was very interested in that, but there was no time and not really competencies [ph?] at the time.  And of course you needed to have a lot of tiny motors .  I did purchase at the time that-- what was it called?  Amazing Baby?  Or there was this doll created by Rod Brooks.</interviewee>
<interviewer>My Real Baby?</interviewer>
<interviewee>It was My Dream Baby and My Real Baby.  You had My Dream Baby that was created by Hasbro, and it was a competition with My Dream Baby.  And so we purchased My Dream Baby to have the facial expression, and we tried to  the same thing, but no.  Yeah, I mean, it stopped there, so didn't get the chance to go further.  Finally, I mean, we created a smaller, simpler doll, which had five degrees of freedom and we produced about 40 of these, which were sold to various museums and educational purpose.  </interviewee>
<interviewer>So in terms of the degrees of freedom and the relation to language, so things like eye direction and pointing and-- what are the essential elements?</interviewer>
<interviewee>Yeah, we tried to look into that, but basically the language at the time was only about sensory input and simple motor commands, so just turning the head left and right, moving the limbs, or have a notion of what is on your left side and on your right side of the body; what does it mean to lift up, so what's this relationship in terms of the central point of your body.  We didn't look at eye-- I mean, direction of the eyes.  We did build-- I should show you that-- we did build small eyes that can rotate and we embedded them with cameras, but we never-- I mean, we never used that for language learning.</interviewee>
<interviewer>How did the field of language grounding evolve from then?</interviewer>
<interviewee>I left it, so I don't know.  It's funny that you ask me that, because I discussed-- I was at  just last week and I met two people from Nick Roy's lab, and I'm so surprised to see that they're moving into more of language now.  There again, , I'm so glad that people are revisiting that in a different viewpoint.  So I think that people left a little bit that field for quite some years, and they're revisiting now this field from more a planning viewpoint,  grounds, language commands.  You also had people in Germany that were looking at using language as a formalism [ph?] to guide the robots through steps in programming by demonstration.  So I haven't followed the field that closely.  My impression is that it didn't progress much as far as robotics is concerned.  There is a lot of work being done more in AI or simulation, but in real robots I think that-- I mean, of course there is the RISE [ph?] group that is doing a lot of work in this respect, and there was Jun Tani's group also that continued--</interviewee>
<interviewer>Luke Steele's has been .</interviewer>
<interviewee>Luke Steele's, but my impression is that he stopped working with robots.  I may be mistaken, again.  I remember where the two talking heads, and then--</interviewee>
<interviewer>I don't know.  I interviewed him, but.</interviewer>
<interviewee>You interviewed him?</interviewee>
<interviewer>I didn't see any robots, but.</interviewer>
<interviewee>No, no.  I think that he moved away from robots.  Yeah.</interviewee>
<interviewer>So when you got to USC and you were working with Maja, what was the project?</interviewer>
<interviewee>So there, we were really focused more on imitation of motion.  So we were interested in designing, again, I mean, a bio-inspired controller that could explain the way you process visual information and then you transform that into motor commands.  So this is closely related to the so-called Carussman [ph?] problem.  It's a very broad topic in programming by demonstration.  It's one thing to interpret other people's motion; it's another thing to actually be able to reproduce this.  Even though we may have the same body structure, there is still a lot of Carussman's issue that needs to be resolved.  So we looked at that from a connectionist viewpoint.  Together with Michael Arbib we also looked at how this could relate to some brain areas and connectivity across different brain areas.  A hot topic at the time was a mirror neuron system, and what role does it have in transforming this information, simplifying to some extent this Carussman's problem, maybe at the level of action rather than the level of detailed motor commands.  So that was the topic.  </interviewee>
<interviewer>And what kind of system did you build to explore that?</interviewer>
<interviewee>We didn't build any system.  We were very much in simulation at the time.    At USC.  We used-- so together with Stefan Schaal, we got the opportunity to go to Japan at ATR in Mitsuo Kawato's group, and use the platform that was called DB.  It's a very power-- or was a very powerful-- humanoid platform, completely hydraulic, that enabled very smooth control of the motion.  So the work was put onto the other platform, but there was no creation-- I mean, there was no hardware design at the time.  In fact, if you contrast-- that's what I said at the beginning.  In Edinburgh, they really put a lot of money to have technicians to build the hardware, but this is very rare, and USC was more of a computer science department, so there was no actual workshop.  So hardware had to be purchased ready-made.</interviewee>
<interviewer>And how long were you at USC?</interviewer>
<interviewee>So, I was at USC for three years.  A little more than three years, yeah.</interviewee>
<interviewer>And then how long were you in Japan?</interviewer>
<interviewee>Oh, in Japan, I visited while I was at USC, so for a period of one month or more.</interviewee>
<interviewer>And did you have other collaborations while you were at USC?</interviewer>
<interviewee>Well, within USC we had a very large network of collaboration.  I will say at large, within the different departments, I think it was-- it was just a fantastic period and everybody recalls it as such.  You had a group from-- for instance, you had Gerry Loeb, who was doing all of this very detailed model of the way motor control is done at the level of the muscles.  And then you had all the department from physical therapy that was looking at how impairment following from stroke resulted in impairment in motor control, especially imitation.  And so there were a lot of collaboration between these departments at large.  And I also collaborated briefly with the University of Notre Dame.    With Steve Boker.  Yeah.</interviewee>
<interviewer>What was Maja like to work with?</interviewer>
<interviewee>I'd like to skip this question.</interviewee>
<interviewer>What was Michael Arbib like to work with?</interviewer>
<interviewee>He is very fascinating in a way.  Really, he's-- I think he's extremely smart and has a very good understanding of the issues at stake that nobody really looked at, that we are passing by all the time.  I think he was one of the best person I had to review my papers.  He is really the best reviewer you could ever had before you actually send your paper out for review.  So it was very enjoyable.</interviewee>
<interviewer>And what was John Holland like as an advisor?</interviewer>
<interviewee>I'm sorry, who?</interviewee>
<interviewer>Holland, as an advisor.</interviewer>
<interviewee>John Hallam?  He was not directly my advisor.  Sorry, John Hallam, not Holland.  Yeah.  </interviewee>
<interviewer>Who was your direct advisor?</interviewer>
<interviewee>So, I mean, it was really Kirsten Dautenhahn and Gillian Hayes.  </interviewee>
<interviewer>What were they like to work with?</interviewer>
<interviewee>Gillian Hayes was-- I mean, is a physicist by training, so she was very helpful in terms of providing guidance in the development of the mathematics of the model.  And of course she was a very gentle and nice person, kind, to work with.  Kirsten Dautenhahn provided a lot of-- just the background that I was interested in as a general aspect in terms of the biology behind imitation learning, she had-- a bit like Michael Arbib-- she had this extensive knowledge that I lacked because I came with this more mathematical background, and she was providing the other view of the topic that I wanted to tackle, which is what do animal do-- how do they come to imitate; what does that mean; what is imitation; what is-- all of these terms that come from psychology or biology or zoology in general.  But she also-- I mean, I think that she was really a pioneer at the time by bringing together robotics and core notion of biology.  She wrote a core paper in '95 which I see a lot of people not necessarily quoting but literally reusing the same ideas, so I don't think this is the best way to show how pioneering the ideas were, because people heard them and maybe they-- -- they forgot who wrote that in the first place.  But they were so powerful and so clear that you see them in most papers these days.  The correspondence problem-- she was the first person to point it out in imitation learning, and you would think it's obvious; everybody talks about it in various ways these days, if people are talking about it in terms of  transfer of force control, impedance control.  At the time she was just talking about it in a much more generic sense, but if you read her paper she was already detailing it at the level of either, indeed, body motion, low-level type of control, or very high-level type of control.  So I think that she was more than inspiring.  I mean, she was-- and she has always been a support to me.</interviewee>
<interviewer>How would you describe the correspondence problem?</interviewer>
<interviewee>  How would I describe the correspondence problem?  Hmm.  Basically it's you look like me but you're not like me, so what's the difference?  </interviewee>
<interviewer>Why is that such a hard problem for robotics?</interviewer>
<interviewee>It's to define the difference and to be able to still find similarities and find some common ground so that we can relate to each other.  So this is not easy, because it's highly task-dependent.  It's body-dependent.  It's context-department.  There is no generic rule.  And robots, this is no generic body; there is no generic kinematic for robots.  We keep building these.  As we all know, just determining the dynamics of a robot is an open problem.  It's called system identification.  So this is the same for humans.  We constantly identify our own dynamics; it changes from day to day, depending on tiredness or other factors.  So this creates a problem.  You have to identify what you want to reproduce in a behavior that you observe from an expert that is teaching you a new behavior, and then you have to learn how to adapt your own controller to achieve the .  And learning how to adapt your control to achieve  is extremely difficult.  This correspondence problem is phrased differently.  People call that also inverse reinforcement learning, for instance.  It's part of defining the cost function-- what is important-- and finding the optimal control policy to achieve this.  So there are different way of phrasing the same problem, but it's a core problem.  It's a core problem in engineering.  Once you've defined the cost function, then you'll say, "Okay, now all the problem is just optimizing the ," assuming that you have a good model-- and optimizing the policy, the controller-- call it what you want.  But there are various ways to optimize that.  And if you made a poor choice regarding a cost function, then everything needs to be redone.  So people are-- I mean, people are discovering now that the two problems are really tightly linked.  And that's why it's very interesting to see that there are people that try to learn at the same time the cost function as they do the controller.  And so the correspondence problem is in effect this issue.  It's two questions.  It's where is the correspondence, and how do I achieve correspondence?</interviewee>
<interviewer>And how does this relate to the challenges of human-robot interaction?</interviewer>
<interviewee>It's core to that, because if you want to interact you need to have a way of exchanging signals that are meaningful both ways.  And this is also related to some extent to language.  I mean, language is there as a way to ground-- even though you have different perception-- to ground them into something that we will agree upon.  So usually we have a single word to define the same thing even though we may perceive it very differently.  So human-robot interaction implies that we interact.  So we will share the same environments-- we may not perceive it the same way-- and we may act together onto the same set of objects.  So what does that really mean?  Because we will have to exchange, interpret other people's behavior, so the robots will have to interpret the human behavior.  To interpret the human behavior needs to have either a good model of the human, or to relate this model to its own behavior.  And so in the particular case of using the human as a teacher for teaching the robot to do things, which is one particular aspect of human-robot interaction, then the correspondence problem takes over its place, like I explained before.</interviewee>
<interviewer>In that regard, is it an easier problem if the robot is a humanoid form, or does that make it harder?</interviewer>
<interviewee>  Of course in a way it's easier.  It's easier because if you can embody the robot, as we say, or doing so-called kinesthetic teaching where you assume that the robot is  so you can actually move the limbs, then it's much easier because usually the human robot will have about the same size as you do, so you could almost transmit each of your drawing's motion to the robot's; you will have just the problem of scaling, to some extent.  If you contrast this with an articulated arm that is planar in its motion-- like most of  robots or some of them that we have in the lab-- then the only way you can actually guide a robot is by using the end-effector, because the drawings-- I mean, the construction of the drawings is so different that it's-- and also the size of the robots, and maybe the robot is also too powerful so it's becoming dangerous to actually guide a robot.  So in this respect, just from a pure experimentation viewpoint, it's easier.  It's also easier for humans to sort of predict the motion of the robots the first time they see it.  At least that's what they expect is that they expect motion that will look similar to human, and that's the idea with human robot is that we will generate motion that are closely related to that of human.  Just the workspace of the robot is similar to the workspace of the human, so you know that the end-effector will move within some sphere and following a continuum that is similar to the continuum that you have seen in human motion.  Or else if you see a robot that doesn't look like a human than you need to look at it for quite some time to get a sense of the kinematic of this robot, its dynamics.  So it's a bit like watching a-- I don't know-- a spacecraft for children, or a car-- simply learning the design.  It's not just any animal.  So it's easier in this respect.  But still it's not easier because the correspondence is still there.  Humanoid robot is still very different from a human.  Joints are usually attached linearly.  We don't have ball joints, clear ball joints, as we simulate them.  Yeah, we don't have this idea of the synergy of muscles for driving the joints.  The size is not the same.  The dexterity is not the same.  So maybe the difference almost subtle, their model level of the dynamics of the control or the range of motion, rather than the kinematic.  So it solves only part of the problem.</interviewee>
<interviewer>And in terms of the language-- I mean, I think intentionality would be a big factor, and that sort of gets you into this problem of other minds and things like that.  Do you worry about those?</interviewer>
<interviewee>No, I think I'd like to skip the question, because intentionality is such a difficult problem, so you are-- I haven't studied really psychology and philosophy, so I stay away from these terms because I'm not sure I have the right definition.</interviewee>
<interviewer>Well, in terms of language grounding, this issue of pointing at something, and the idea that an animal or a robot would pay attention to the object that you're pointing at instead of the end of your finger-- sort of making that leap about--</interviewer>
<interviewee>So your question is do we get--</interviewee>
<interviewer>Is that the kind of problem when you're dealing with language learning in these correspondence systems that you try to deal with?  Like how do you get from pointing at something instead of just holding out a finger?</interviewer>
<interviewee>We don't.  We build that in.    I know it's a stage in development in children that is very important, and that-- I mean, it's something that differentiates some species from other species in terms of the complexity of their understanding and, yes, this is related to the theory of mind.  Yeah, I think that we are very far from that in robotics at this point in time.  So we haven't addressed that properly.  Either we provide a robot with already this competence of not looking at the finger but looking at where the finger is pointing at.  So that we did at some point in the lab; we looked at that from just a probabilistic matter.  I mean, you will use the direction that is being pointed at as one way to narrow the attention of the robot, so to decrease the amount of information that a robot is provided with by simply looking at-- -- at the intersection between the cone-- if you look at your finger and you added some uncertainty then you can look at this as a cone; then you look at the intersection between this cone and the first planar surface that you find that has some interesting properly or either it's endowed with objects that could be interesting.  But again, all of that is built in.  So it's providing the robot with a lot of prior knowledge as to what is interesting in the world.  So it's bypassing several stages of development.</interviewee>
<interviewer>And over the time that you've been working on this problem, have you seen an evolution in the sort of complexity and the sophistication of the approaches?  What have been sort of the milestones of problems you've solved, and then sort of built more on another level?</interviewer>
<interviewee>Well, I realize that instead of growing in terms of the complexity-- as we call  of complexity-- of the competencies, I'm going the other way around.  I'm going lower and lower level by the day.    So it's growing in complexity but in different way.  So I started with language, which is very symbolic, very high-level type of understanding.  Went down to having sequence of motion, so you will say moving away from language to just pure motion.  And now we are at the level of just doing force control.    So we are just trying to get a robot to solve the correspondence problem for doing a simple trajectory.  And it seems so basic, but I really want to understand this well before I go back up, and to try to have a very good understanding of basic law of motion and how we control our body and how we can use this for also perceiving motions of other entities.  So it's sort of growing in the complexity of the low-level controller that we have in our brain or elsewhere that we need to endow the robot with, and make sure that this piece is well understood before climbing back the ladder and then going to more complex type of interaction.  So.</interviewee>
<interviewer>And in doing that, do you look a lot to human and animal models of development?</interviewer>
<interviewee>Unfortunately, no.  We are moving away from that, because very little is known about how animals really control-- I mean humans control their body.  And to some extent-- well, that's not true, actually.  Scrub that.  Because it's not really-- we still use the human all the time as the teacher, and we try to infer a level of control, even at the level of force control, from the human.  But if you take simple impedance controller, there is a lot of debate as to whether a human actually controls their muscles and their limbs this way.  So I will say we use state-of-the-art type of controller in robotics and use the human as a way of identifying the open parameters.  But that doesn't mean that the human is actually using that for controlling his or her own body.  I mean, we do look also at doing computational model of the way human process information, so that's always one part of the work that we do in the lab.  So, recently we had Biljana Petreska who just graduated; she's at Harvard now as a postdoc.  And she did very interesting model of the way the brain is affected after lesion following from stroke, and how this lead to apraxia, and apraxia is one particular form of inability to control one's limb.  You put in a request-- so it's  formation of imitation.  So, yeah, we do a little bit, of course.</interviewee>
<interviewer>And it's very much on sort of the boundary between perception and motion, right?  I mean, you have really both sides of the problem.  You have all the problems of perception and all the problems of control.</interviewer>
<interviewee>Yes, but it's tightly linked, especially if you think in terms of  control.  I mean, there is the perception of the external world and there is the perception of your own motion, and they probably rely on the same mechanism.  So yes, we tend to think that this is a continuum and not two separate process.    Sorry, I think we are drifting into--</interviewee>
<interviewer>Yeah, yeah, yeah.  So you mentioned a student who just went to Harvard.  Who have been some of your students, and where have they gone?</interviewer>
<interviewee>So, Sylvain Calinon is a senior researcher at the Italian Institute of Technology.  I think he's by now very well respected for the work that he did during his thesis in programming by demonstration, especially tackling one part of this correspondence at large, the so-called "what to imitate"-- so what is important to extract in a demonstration; what should you pay attention to.  He got a couple of awards for his studies in this respect.  So, I mean, these are two major people.</interviewee>
<interviewer>And who are some other people you've collaborated with?</interviewer>
<interviewee>I collaborate currently with Jacqueline Nadel, who is a psychologist, but this is a different topic in the lab, where we developed a wearable camera that you put on the forehead of children to monitor their gaze behavior.  This has application possibly for early diagnosis of autism.  So trying to monitor whether children very early on tend to not look directly at other people's eyes, which is one particular deficit that is relatively widespread in the autism spectrum disorder.  That's it.  </interviewee>
<interviewer>And in terms of funding, where do you-- who's been sponsoring your research?</interviewer>
<interviewee>Right, of course.  So, in Europe, we get a lot of funding from the European Commission, and of course this each time entails collaboration with various people in Europe, but also in the world.  So I should have mentioned that we've been collaborating now for three years also with AIST and the JRL Lab in AIST with Abdurahman Khadr, who's a director there.  And we worked with the HRP-2 platform on developing control for enabling a robot and a human to carry collaboratively a load.  So we get a lot of funding from the European Commission.  I will say two-third of the funding comes from the European Commission.  The rest comes from the Swiss National Science Foundation.  We also get a little bit of funding from industry, but it is very minor.</interviewee>
<interviewer>And was that similar when you were at Edinburgh?</interviewer>
<interviewee>When I was at Edinburgh, I had a scholarship.  Yeah.</interviewee>
<interviewer>And also at USC?</interviewer>
<interviewee>At USC-- no, I was paid by USC.  But I was paid by the National Science Foundation-- the U.S. National Science Foundation at USC.</interviewee>
<interviewer>So what are some of the other humanoids that you've been working with?</interviewer>
<interviewee>I think I listed all of them.  There is the iCub, HRP-2, the DB robot, and of course the robot that we built here.  So Robota, we have various version of this.  And the Fujitsu HOAP-2 and HOAP-3 robot.  And-- yep.  That's it.</interviewee>
<interviewer>What do you think of the iCub?</interviewer>
<interviewee>I think it's a wonderful platform. It's really unique.    It's funny, because before we went onto having robot Cub, the first project that created this-- I mean, we had a conversation over dinner at AISB [ph?] in-- what was it?-- 2003, I think.  It was Giulio Sandini, is the person who really created this.  And we brainstormed over the dinner that it would be so wonderful to have a humanoid platform that will be completely open source, both at the level of the hardware and the software, because we so much needed that.  At the time we had only Honda; you had the QRIO robot, but they were protected; nobody could actually access the hardware, so it was impossible to duplicate it.  They cost a lot of money.  And so we thought it was really hindering research in human robots, and we needed to have that.  And then we got lucky.  I mean, there was this funding that came, and Giulio and his team  did a great work building this platform with other people.  And so now you have-- this robot is being duplicated lots of places.  Of course it's a prototype, so I won't  very frequently.    We fix that all the time, but I don't mind, because it offers an opportunity to explore a complexity in terms of hardware and control that is not existing for any other humanoid robot, as far as I know.  And there are great platform to make a robot is great, and I think this is the closest platform in terms of complexity, but it doesn't have legs.  It doesn't allow you to have the full scope of motion.  You have this bigger platform, like HRP-2  or now the new robot that they have at TTR, which has .  But again, they're so incredibly expensive that very few labs can have them, and they don't have this open source framework.  So we benefit a lot from this open source because we post, of course, the code that we develop all the time, but we also benefit from the code that is being posted by other people.  We were lucky also to be part of different project that improved the current version of the robot, and the new project on endowing the iCub with skin, tactile sensing.  I mean, it's offered us the ability to explore new ways of doing manipulation without having the classical forced closure approach, which is-- what I love in the iCub is that it's inherently noisy and it breaks, and I love this, because that forces us to really tackle the complexity of having very robust controller, not something that relies on perfect sensing and perfect actuation.  The type of control that we have in humans-- our sensing is extremely noisy.  Even our actuation is very noisy.  People never over the same trials repeat exactly the same kinematic.  Still, we manage very precise, skillful behavior.  And so the iCub, in this respect, offers the complexity in terms of the number of degrees of freedom and the complexity in terms of the sensing and the actuation, that is really forcing research to go one step forward in designing controllers that are truly robust.</interviewee>
<interviewer>In terms of the open source, is it an open source operating system, the kind that the Willow Garage kind of brought us, or is it a different model?</interviewer>
<interviewee>Yeah.  In fact, as far as I understand, YARP and ROS are very similar.  In fact, because we were in two European project, one needed to have everything coded in YARP, the other in ROS.    We are forced to now pass from one to the other all the time in the lab.  And so we have , and it's relatively straightforward.  So that's the same principle they have, SVN-- is it SVN?-- repository, where you can post all of the codes.  It's fully documented.  So YARP is sort of this-- I'm not really a computer scientist, so I don't know what the terms are, but it's really sort of middleware.  It's very similar to ROS, offers the same type of modularity.  I think that sort of the major difference between ROS and YARP is -- we don't have-- I guess the YARP people don't have as much money as the ROS people, so they don't advertise as much.  And I think it would be sad if they were not to collaborate in the long run.  My understanding is that ROS was to some extent the revision of YARP, but don't quote me on that.  So it's similar.  I think the advantage of ROS is that a lot of people now are really posting a lot of code for also Vision [ph?].  Not necessarily directly robotics, a lot just for the sensing part.  And we've benefited from that in many ways here in the lab.  I guess most of the codes right now that is being developed for the iCub under YARP is really dedicated for the iCub platform, whereas ROS seems to be applied to more type of robotic platform than just the PR2.  There is the one Barrett one, I think, at least, and maybe other platform.  </interviewee>
<interviewer>Great.  And what about QRIO?  Is that a good humanoid platform or does that have limitations that the iCub won't?</interviewer>
<interviewee>I wanted to have a QRIO, but as you know they stopped producing these.  It's finished, since they changed CEO two years ago and they closed down the Sony Research Lab.  So I don't know.  It seemed that it was a fantastic platform, but nobody really could-- had the chance to really use it.</interviewee>
<interviewer>Yeah, some things happened with the Sony humanoid as well.</interviewer>
<interviewee>Say that again?</interviewee>
<interviewer>The Sony humanoid for the--</interviewer>
<interviewee>But that's the QRIO.  QRIO is the Sony.</interviewee>
<interviewer>Oh, that's the Sony.</interviewer>
<interviewee>Yeah, that's the Sony one.  Yeah.  Maybe you meant the Fujitsu HOAP?  So the HOAP-2 and HOAP-2 robot, or the Nao, Aldebaran?</interviewee>
<interviewer>Nao.  Yeah, the little--</interviewer>
<interviewee>So, we never got a chance to really work with Nao, but we know well Aldebaran Robotics.  I mean, all the feedback I heard from people using it is that it's a very variable platform and very-- I mean, very-- well, very well made.  I think people use it mostly for soccer, right?  So we didn't purchase it because it's not meant for manipulation and we were really doing a proper control, and so it's small, so it's not a  platform; it's just not meant for that.  You know that they're designing a new platform which will be called Romeo [ph?].  And I believe it will be about one-meter-thirty centimeter.  So in this respect, it's probably sort of a replacement of the QRIO.  Or not.  The Fujitsu HOAP-2 and HOAP-3 were fantastic platform.  They really worked immediately on the first day of use.  People came here, they installed the PC, it was open source, running real-time .  This is the easiest platform we've ever used.  We had never a single problem, not even on the level of the hardware.  And I really insist on that, because I cannot say the same about much more expensive platforms, such as the Barrett.    Which is a real problem.</interviewee>
<interviewer>And what do you see as the big problems in the future of human-robot interaction and imitation and correspondence?</interviewer>
<interviewee>That's a tough question.  I don't know.  I'm not good answering these questions.  Can I skip?</interviewee>
<interviewer>Sure.  Then I'll just ask you what do you think are going to be the big applications and breakthroughs in robotics?</interviewer>
<interviewee>Yeah, but these are the type of questions-- .  You really need to have that from everyone?</interviewee>
<interviewer>It's one of my questions.  I mean, where do you see it going?  What do you think is going to be the exciting problems that you want to work on in the next few years?</interviewer>
<interviewee>I mean, it's not-- the current breakthrough that we are living through is really-- and doing all of these platforms with tactile sensing.  I think this really changes a lot the type of interaction we can have with robot.  There are so many different groups that are trying to develop now tactile sensing that is very modular, which you could then place on arbitrary type of robot.  So if-- and I'm pretty certain that this will happen within the next few years, because there are also companies that are working on this.  And this will provide so much more flexibility in terms of the control, starting with just the robot being able to discover when it's hitting itself-- as basic as that.    When it's also-- there are people working on technology that can differentiate between touching an object, non-living object, and a living object, meaning sensing temperature, either from a distance or directly through direct contact.  I mean, this  offers one possibility for a robot to distinguish between hitting an obstacle and hitting a human, and then acting accordingly, which is things that people have looked at in the framework of impedance control, but from purely sensing forces, and this is much more natural.  Again, if you endow the complete body with this, this will be very important in human-robot interaction.  It will provide some level of safety and coherence in the behavior.  So this is a breakthrough in sensing that we are undergoing right now, and will probably offer a lot of avenues of research within the next, I don't know, 25 years.  Following that, I don't know.  I personally am very intrigued-- you mentioned facial expression.  I'm very intrigued by what people manage to do in terms of complex facial expression and expression of emotion or recognition of emotion in others.  I've never done it myself-- it's bizarre, because people contact me sometimes to review papers on this.  I've never, ever written on that topic, ever.  I've expressed interest.  I don't know why people that I should be doing that.  But I'm intrigued, and we'll try to see how much this could be used in the framework of the interaction with humans.  So when the human teaches a robot, it's often difficult to see what the robot is really learning from what you're teaching, especially as you go lower and lower level.  When you teach information such as force, these are not information that you can really spell out.  You cannot tell the robot, "Apply 10 newton meter more."  However you can say, "Move 10 centimeter away from this."  So it's-- so the notion of force is there.  We can show the right force to be applied.  We have a very clear idea of where the force is, but we cannot express it.  And now the problem is: Is the robot now interpreting correctly what I'm saying, and how could the robot express that back in a way that I could understand it?  And so we may look at facial expression in this respect and maybe-- my feeling is that maybe there is a little bit of a split between people doing more emotion-based, psychology type of work in robotics, and the pure control people, and they talk to each other.  It's a bit like splitting between pure technical approach and human science, and I don't know why there is such a split.  And so possibly if people manage to convince each other that they're doing valuable research then this will be probably a step forward for robotics.  But how to bridge that, I have no idea, and this will take years.  There are a lot of people that believe in flexible actuators, passive compliance, etcetera.  So, it seems to be the way to go these days.</interviewee>
<interviewer>In terms of the tactile sensing, do you think the big technological hurdles are primarily the materials or the integration of the sensory data?  Or a bit of both?</interviewer>
<interviewee>It's probably both, and the interpretation of the data.  Because you have tons of it, then you are really-- you have again the "what" question.  What is important, and when, and what for?  So I mean, from a technology viewpoint, there are so many people working on this that suddenly they will find various solutions very rapidly.  I mean, capacity sensing is  and there are plenty of solutions these days.  There are people looking at how to sense stretch, which is something that is very useful to .  So I think the challenge are, at both level, as you improve the hardware and provide more and more information, whether it's temperature, the direction of the forces, the flexibility of the material-- flexible material means also that you don't want it to break, to have this flexibility and keep this flexibility over time, like the skin does.  Replacing these-- I'm scared what will happen if we have these robot at a  and then one patch breaks.  Can we just plug in in-- plug and play?  So yeah, there is this aspect, and there is a sensing aspect, and just providing all this information in real time.  And then finally there is the interpretation of this information.  In real-time.    So, yeah, that's why there is room for research in the years ahead.</interviewee>
<interviewer>So young people who might be interested in robotics research, what would you recommend?  What would you advise them to study?</interviewer>
<interviewee>You mean prior-- so what type of undergraduate studies they should be doing?  My advice is really to go to-- I mean, I think without a strong background in math, you will need feel comfortable.  So my advice is to take core engineering degrees.  Doesn't mean that they cannot do a secondary degree in psychology afterwards or other things, but this is not something that you read at bedtime.    You have to have taken classes.  You have to have been trained.  And if you don't have that, our problem is that often you find works that are not-- where people simply don't have the background and sort of miss out on simple solutions from just the mathematical standpoint.  I mean, sometimes they-- math is just math.  It's not difficult, but it takes time to understand a mathematical equation.  You have to get used to that.  It's like a new language.  So you have to train yourself.  And once you're familiar with this, then it's easy to understand it.  And I see too frequently that with some of the computer science degree that are being given out, they're somewhat too light in terms of the math studies provided, and ultimately the graduate student suffers from this, because-- yeah, because they just don't know where to look for the answer, how to train themselves.  It's too late.  So that's my advice.  It's strong advice.  I know it's tough, but it's really to their advantage.  It's only to their advantage.</interviewee>
<interviewer>And how did you make that sort of switch back from graduate school-- from physics into interest in neural networks and psychology?</interviewer>
<interviewee>But I did my master at CERN, which is the European center for particle physics, so I specialized in particle physics.  And at the time, people were using artificial networks as an alternative to other statistical technique for extracting out of the massive data that we're getting one particular peak that will represent one particular split in the particles.  And bizarrely enough, artificial networks were not being taught to physicists, even though most of the theory has been developed by physicist.  So it was not part of the curriculum, and so I did not feel comfortable using this tool, just as a tool, without understanding the math behind that.  And so I went to do a master's thesis of the University of Edinburgh because they were offering a master's in so-called knowledge-based system that was teaching the basic math behind all of these so-called artificial intelligence type of technique that were viewed as alternative to other classical statistical technique to analyze multidimensional data sets.  So I went to Edinburgh for doing this master, and one of the class that was compulsory there was a class on robotics.    And that was taught by John Hallam.  And I spent hours building a robot with my partner at the time.  We won the competition.  And so it was implementing a type of non-connectionist controller, and I thought I don't want to just do physics any longer.  I want to build things also.    So that's where I went to robotics.  Yeah.</interviewee>
<interviewer>Is there anything we missed or anything you'd like to add?</interviewer>
<interviewee>No, I think as far as I'm concerned I'm good.</interviewee>
<interviewer>Good.  Well, thank you very much.</interviewer>
</interview>
</subject>
